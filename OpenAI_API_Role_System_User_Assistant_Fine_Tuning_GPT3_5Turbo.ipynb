{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/isaac-mackey/mind-uploading/blob/main/Isaac_SMS_Role_System_User_Assistant_Fine_Tuning_GPT3_5Turbo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Installs and Imports"
      ],
      "metadata": {
        "id": "mSrCM17zfIOx"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SVoU7VrzGzxn"
      },
      "outputs": [],
      "source": [
        "!pip install openai\n",
        "!pip install numpy\n",
        "!pip install tiktoken"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import openai\n",
        "import csv\n",
        "import json\n",
        "import os\n",
        "import numpy as np\n",
        "from collections import defaultdict\n",
        "import tiktoken"
      ],
      "metadata": {
        "id": "WysuPFYQHfCq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "yL9UkuK1g_iT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Validate the dataset for training"
      ],
      "metadata": {
        "id": "aHF-4-thfWnD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#from OpenAI website to format data;  https://platform.openai.com/docs/guides/fine-tuning/preparing-your-dataset\n",
        "# !openai tools fine_tunes.prepare_data -f /content/drive/My\\ Drive/sms-20231224020008.xml-role-system-user-4.json\n",
        "\n",
        "# Next, we specify the data path and open the JSONL file\n",
        "\n",
        "# data_path = \"/content/drive/My Drive/in-sent-after-2023-07-01-before-2024-01-12.mbox-5.json\"\n",
        "data_path = '/content/drive/My Drive/sms-combined.xml-role-system-user-7.json'\n",
        "\n",
        "# Load dataset\n",
        "with open(data_path) as f:\n",
        "    dataset = [json.loads(line) for line in f]\n",
        "\n",
        "# We can inspect the data quickly by checking the number of examples and the first item\n",
        "\n",
        "# Initial dataset stats\n",
        "print(\"Num examples:\", len(dataset))\n",
        "print(\"First example:\")\n",
        "for message in dataset[0][\"messages\"][:5]:\n",
        "    print(message)\n",
        "\n",
        "print('\\n'*3)\n",
        "\n",
        "# Now that we have a sense of the data, we need to go through all the different examples and check to make sure the formatting is correct and matches the Chat completions message structure\n",
        "\n",
        "# Format error checks\n",
        "format_errors = defaultdict(int)\n",
        "\n",
        "for ex in dataset:\n",
        "    if not isinstance(ex, dict):\n",
        "        format_errors[\"data_type\"] += 1\n",
        "        continue\n",
        "\n",
        "    messages = ex.get(\"messages\", None)\n",
        "    if not messages:\n",
        "        format_errors[\"missing_messages_list\"] += 1\n",
        "        continue\n",
        "\n",
        "    for message in messages:\n",
        "        if \"role\" not in message or \"content\" not in message:\n",
        "            format_errors[\"message_missing_key\"] += 1\n",
        "\n",
        "        if any(k not in (\"role\", \"content\", \"name\") for k in message):\n",
        "            format_errors[\"message_unrecognized_key\"] += 1\n",
        "\n",
        "        if message.get(\"role\", None) not in (\"system\", \"user\", \"assistant\"):\n",
        "            format_errors[\"unrecognized_role\"] += 1\n",
        "\n",
        "        content = message.get(\"content\", None)\n",
        "        if not content or not isinstance(content, str):\n",
        "            format_errors[\"missing_content\"] += 1\n",
        "\n",
        "    if not any(message.get(\"role\", None) == \"assistant\" for message in messages):\n",
        "        format_errors[\"example_missing_assistant_message\"] += 1\n",
        "\n",
        "if format_errors:\n",
        "    print(\"Found errors:\")\n",
        "    for k, v in format_errors.items():\n",
        "        print(f\"{k}: {v}\")\n",
        "else:\n",
        "    print(\"No errors found\")\n",
        "\n",
        "# Beyond the structure of the message, we also need to ensure that the length does not exceed the 4096 token limit.\n",
        "\n",
        "# Token counting functions\n",
        "encoding = tiktoken.get_encoding(\"cl100k_base\")\n",
        "\n",
        "# not exact!\n",
        "# simplified from https://github.com/openai/openai-cookbook/blob/main/examples/How_to_count_tokens_with_tiktoken.ipynb\n",
        "def num_tokens_from_messages(messages, tokens_per_message=3, tokens_per_name=1):\n",
        "    num_tokens = 0\n",
        "    for message in messages:\n",
        "        num_tokens += tokens_per_message\n",
        "        for key, value in message.items():\n",
        "            num_tokens += len(encoding.encode(value))\n",
        "            if key == \"name\":\n",
        "                num_tokens += tokens_per_name\n",
        "    num_tokens += 3\n",
        "    return num_tokens\n",
        "\n",
        "def num_assistant_tokens_from_messages(messages):\n",
        "    num_tokens = 0\n",
        "    for message in messages:\n",
        "        if message[\"role\"] == \"assistant\":\n",
        "            num_tokens += len(encoding.encode(message[\"content\"]))\n",
        "    return num_tokens\n",
        "\n",
        "def print_distribution(values, name):\n",
        "    print(f\"\\n#### Distribution of {name}:\")\n",
        "    print(f\"min / max: {min(values)}, {max(values)}\")\n",
        "    print(f\"mean / median: {np.mean(values)}, {np.median(values)}\")\n",
        "    print(f\"p5 / p95: {np.quantile(values, 0.1)}, {np.quantile(values, 0.9)}\")\n",
        "\n",
        "# Last, we can look at the results of the different formatting operations before proceeding with creating a fine-tuning job:\n",
        "\n",
        "# Warnings and tokens counts\n",
        "n_missing_system = 0\n",
        "n_missing_user = 0\n",
        "n_messages = []\n",
        "convo_lens = []\n",
        "assistant_message_lens = []\n",
        "\n",
        "MAX_TOKENS_PER_EXAMPLE = 2048\n",
        "\n",
        "dataset_split_long_messages = []\n",
        "for ex in dataset:\n",
        "    messages = ex[\"messages\"]\n",
        "    if num_tokens_from_messages(messages) > MAX_TOKENS_PER_EXAMPLE:\n",
        "        sublists = []\n",
        "        current_sublist = []\n",
        "        system_message = messages[0]\n",
        "        current_sum = 0\n",
        "\n",
        "        for message in messages:\n",
        "            if current_sum + num_tokens_from_messages([message]) > MAX_TOKENS_PER_EXAMPLE:\n",
        "                # Current sublist reached its maximum capacity without exceeding target sum\n",
        "                sublists.append(current_sublist)\n",
        "                current_sublist = [system_message, message]       # Start a new sublist\n",
        "                current_sum = num_tokens_from_messages([message])\n",
        "            else:\n",
        "                current_sublist.append(message)\n",
        "                current_sum += num_tokens_from_messages([message])\n",
        "\n",
        "        # Adding the last sublist if it's not empty\n",
        "        if current_sublist:\n",
        "            sublists.append(current_sublist)\n",
        "\n",
        "        for s in sublists:\n",
        "            dataset_split_long_messages.append({\"messages\":s})\n",
        "\n",
        "    else:\n",
        "        dataset_split_long_messages.append(ex)\n",
        "\n",
        "dataset = dataset_split_long_messages\n",
        "\n",
        "print(\"Num examples in dataset\",str(len(dataset)))\n",
        "\n",
        "for ex in dataset:\n",
        "    messages = ex[\"messages\"]\n",
        "    if not any(message[\"role\"] == \"system\" for message in messages):\n",
        "        n_missing_system += 1\n",
        "    if not any(message[\"role\"] == \"user\" for message in messages):\n",
        "        n_missing_user += 1\n",
        "    n_messages.append(len(messages))\n",
        "    convo_lens.append(num_tokens_from_messages(messages))\n",
        "    assistant_message_lens.append(num_assistant_tokens_from_messages(messages))\n",
        "\n",
        "print(\"Num examples missing system message:\", n_missing_system)\n",
        "print(\"Num examples missing user message:\", n_missing_user)\n",
        "print_distribution(n_messages, \"num_messages_per_example\")\n",
        "print_distribution(convo_lens, \"num_total_tokens_per_example\")\n",
        "print_distribution(assistant_message_lens, \"num_assistant_tokens_per_example\")\n",
        "n_too_long = sum(l > MAX_TOKENS_PER_EXAMPLE for l in convo_lens)\n",
        "print(f\"\\n{n_too_long} examples may be over the token limit, they will be truncated during fine-tuning\")\n",
        "\n",
        "# Pricing and default n_epochs estimate\n",
        "# MAX_TOKENS_PER_EXAMPLE = 4096\n",
        "\n",
        "MIN_TARGET_EXAMPLES = 100\n",
        "MAX_TARGET_EXAMPLES = 25000\n",
        "TARGET_EPOCHS = 3\n",
        "MIN_EPOCHS = 1\n",
        "MAX_EPOCHS = 25\n",
        "\n",
        "n_epochs = TARGET_EPOCHS\n",
        "n_train_examples = len(dataset)\n",
        "if n_train_examples * TARGET_EPOCHS < MIN_TARGET_EXAMPLES:\n",
        "    n_epochs = min(MAX_EPOCHS, MIN_TARGET_EXAMPLES // n_train_examples)\n",
        "elif n_train_examples * TARGET_EPOCHS > MAX_TARGET_EXAMPLES:\n",
        "    n_epochs = max(MIN_EPOCHS, MAX_TARGET_EXAMPLES // n_train_examples)\n",
        "\n",
        "n_billing_tokens_in_dataset = sum(min(MAX_TOKENS_PER_EXAMPLE, length) for length in convo_lens)\n",
        "print(f\"Dataset has ~{n_billing_tokens_in_dataset} tokens that will be charged for during training\")\n",
        "print(f\"By default, you'll train for {n_epochs} epochs on this dataset\")\n",
        "print(f\"By default, you'll be charged for ~{n_epochs * n_billing_tokens_in_dataset} tokens\")\n",
        "\n",
        "# Calculate the estimated cost for fine-tuning\n",
        "cost_per_100k_tokens = 0.80  # Cost for every 100,000 tokens\n",
        "estimated_cost = ((n_epochs * n_billing_tokens_in_dataset) / 100000) * cost_per_100k_tokens\n",
        "print(f\"Estimated cost for fine-tuning: approximately ${estimated_cost:.2f}\") #I added this for actual cost based on current pricing"
      ],
      "metadata": {
        "id": "-NNdePOiKuKW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Split data into training and testing files"
      ],
      "metadata": {
        "id": "0KGFGpuLY1OR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "\n",
        "#from OpenAI website to format data;  https://platform.openai.com/docs/guides/fine-tuning/preparing-your-dataset\n",
        "# !openai tools fine_tunes.prepare_data -f /content/drive/My\\ Drive/sms-20231224020008.xml-role-system-user-4.json\n",
        "\n",
        "# Next, we specify the data path and open the JSONL file\n",
        "\n",
        "data_path = '/content/drive/My Drive/sms-combined.xml-role-system-user-7.json'\n",
        "data_path = \"/content/drive/My Drive/in-sent-after-2023-07-01-before-2024-01-12.mbox-5.json\"\n",
        "\n",
        "# Load dataset\n",
        "with open(data_path) as f:\n",
        "    dataset = [json.loads(line) for line in f]\n",
        "\n",
        "# We can inspect the data quickly by checking the number of examples and the first item\n",
        "\n",
        "# Initial dataset stats\n",
        "print(\"Num examples:\", len(dataset))\n",
        "print(\"First example:\")\n",
        "for message in dataset[0][\"messages\"][:5]:\n",
        "    print(message)\n",
        "\n",
        "print('\\n'*3)\n",
        "\n",
        "# Now that we have a sense of the data, we need to go through all the different examples and check to make sure the formatting is correct and matches the Chat completions message structure\n",
        "\n",
        "# Format error checks\n",
        "format_errors = defaultdict(int)\n",
        "\n",
        "for ex in dataset:\n",
        "    if not isinstance(ex, dict):\n",
        "        format_errors[\"data_type\"] += 1\n",
        "        continue\n",
        "\n",
        "    messages = ex.get(\"messages\", None)\n",
        "    if not messages:\n",
        "        format_errors[\"missing_messages_list\"] += 1\n",
        "        continue\n",
        "\n",
        "    for message in messages:\n",
        "        if \"role\" not in message or \"content\" not in message:\n",
        "            format_errors[\"message_missing_key\"] += 1\n",
        "\n",
        "        if any(k not in (\"role\", \"content\", \"name\") for k in message):\n",
        "            format_errors[\"message_unrecognized_key\"] += 1\n",
        "\n",
        "        if message.get(\"role\", None) not in (\"system\", \"user\", \"assistant\"):\n",
        "            format_errors[\"unrecognized_role\"] += 1\n",
        "\n",
        "        content = message.get(\"content\", None)\n",
        "        if not content or not isinstance(content, str):\n",
        "            format_errors[\"missing_content\"] += 1\n",
        "\n",
        "    if not any(message.get(\"role\", None) == \"assistant\" for message in messages):\n",
        "        format_errors[\"example_missing_assistant_message\"] += 1\n",
        "\n",
        "if format_errors:\n",
        "    print(\"Found errors:\")\n",
        "    for k, v in format_errors.items():\n",
        "        print(f\"{k}: {v}\")\n",
        "else:\n",
        "    print(\"No errors found\")\n",
        "\n",
        "# Beyond the structure of the message, we also need to ensure that the length does not exceed the 4096 token limit.\n",
        "\n",
        "# Token counting functions\n",
        "encoding = tiktoken.get_encoding(\"cl100k_base\")\n",
        "\n",
        "# not exact!\n",
        "# simplified from https://github.com/openai/openai-cookbook/blob/main/examples/How_to_count_tokens_with_tiktoken.ipynb\n",
        "def num_tokens_from_messages(messages, tokens_per_message=3, tokens_per_name=1):\n",
        "    num_tokens = 0\n",
        "    for message in messages:\n",
        "        num_tokens += tokens_per_message\n",
        "        for key, value in message.items():\n",
        "            num_tokens += len(encoding.encode(value))\n",
        "            if key == \"name\":\n",
        "                num_tokens += tokens_per_name\n",
        "    num_tokens += 3\n",
        "    return num_tokens\n",
        "\n",
        "def num_assistant_tokens_from_messages(messages):\n",
        "    num_tokens = 0\n",
        "    for message in messages:\n",
        "        if message[\"role\"] == \"assistant\":\n",
        "            num_tokens += len(encoding.encode(message[\"content\"]))\n",
        "    return num_tokens\n",
        "\n",
        "def print_distribution(values, name):\n",
        "    print(f\"\\n#### Distribution of {name}:\")\n",
        "    print(f\"min / max: {min(values)}, {max(values)}\")\n",
        "    print(f\"mean / median: {np.mean(values)}, {np.median(values)}\")\n",
        "    print(f\"p5 / p95: {np.quantile(values, 0.1)}, {np.quantile(values, 0.9)}\")\n",
        "\n",
        "# Last, we can look at the results of the different formatting operations before proceeding with creating a fine-tuning job:\n",
        "\n",
        "# Warnings and tokens counts\n",
        "n_missing_system = 0\n",
        "n_missing_user = 0\n",
        "n_missing_assistant = 0\n",
        "n_messages = []\n",
        "convo_lens = []\n",
        "assistant_message_lens = []\n",
        "\n",
        "MAX_TOKENS_PER_EXAMPLE = 2048\n",
        "\n",
        "dataset_split_long_messages = []\n",
        "\n",
        "for ex in dataset:\n",
        "    messages = ex[\"messages\"]\n",
        "    if num_tokens_from_messages(messages) > MAX_TOKENS_PER_EXAMPLE:\n",
        "        sublists = []\n",
        "        current_sublist = []\n",
        "        system_message = messages[0]\n",
        "        current_sum = 0\n",
        "\n",
        "        for message in messages:\n",
        "            if current_sum + num_tokens_from_messages([message]) > MAX_TOKENS_PER_EXAMPLE:\n",
        "                # Current sublist reached its maximum capacity without exceeding target sum\n",
        "                sublists.append(current_sublist)\n",
        "                current_sublist = [system_message, message]       # Start a new sublist\n",
        "                current_sum = num_tokens_from_messages([message])\n",
        "            else:\n",
        "                current_sublist.append(message)\n",
        "                current_sum += num_tokens_from_messages([message])\n",
        "\n",
        "        # Adding the last sublist if it's not empty\n",
        "        if current_sublist:\n",
        "            sublists.append(current_sublist)\n",
        "\n",
        "        for s in sublists:\n",
        "            dataset_split_long_messages.append({\"messages\":s})\n",
        "\n",
        "    else:\n",
        "        dataset_split_long_messages.append(ex)\n",
        "\n",
        "dataset = dataset_split_long_messages\n",
        "\n",
        "dataset_train = []\n",
        "dataset_test = []\n",
        "\n",
        "for d in dataset:\n",
        "    if not any(message[\"role\"] == \"assistant\" for message in d['messages']):\n",
        "        continue\n",
        "    if random.random() < .8:\n",
        "        dataset_train.append(d)\n",
        "    else:\n",
        "        dataset_test.append(d)\n",
        "\n",
        "print(\"Num examples in training dataset\",str(len(dataset_train)))\n",
        "print(\"Num examples in testing dataset\",str(len(dataset_test)))\n",
        "\n",
        "for ex in dataset_train:\n",
        "    messages = ex[\"messages\"]\n",
        "    if not any(message[\"role\"] == \"system\" for message in messages):\n",
        "        n_missing_system += 1\n",
        "    if not any(message[\"role\"] == \"user\" for message in messages):\n",
        "        n_missing_user += 1\n",
        "    if not any(message[\"role\"] == \"assistant\" for message in messages):\n",
        "        n_missing_assistant += 1\n",
        "    n_messages.append(len(messages))\n",
        "    convo_lens.append(num_tokens_from_messages(messages))\n",
        "    assistant_message_lens.append(num_assistant_tokens_from_messages(messages))\n",
        "\n",
        "print(\"Num examples missing system message:\", n_missing_system)\n",
        "print(\"Num examples missing user message:\", n_missing_user)\n",
        "print(\"Num examples missing assistant message:\", n_missing_assistant)\n",
        "print_distribution(n_messages, \"num_messages_per_example\")\n",
        "print_distribution(convo_lens, \"num_total_tokens_per_example\")\n",
        "print_distribution(assistant_message_lens, \"num_assistant_tokens_per_example\")\n",
        "n_too_long = sum(l > MAX_TOKENS_PER_EXAMPLE for l in convo_lens)\n",
        "print(f\"\\n{n_too_long} examples may be over the token limit, they will be truncated during fine-tuning\")\n",
        "\n",
        "# Pricing and default n_epochs estimate\n",
        "# MAX_TOKENS_PER_EXAMPLE = 4096\n",
        "\n",
        "MIN_TARGET_EXAMPLES = 100\n",
        "MAX_TARGET_EXAMPLES = 25000\n",
        "TARGET_EPOCHS = 3\n",
        "MIN_EPOCHS = 1\n",
        "MAX_EPOCHS = 25\n",
        "\n",
        "n_epochs = TARGET_EPOCHS\n",
        "n_train_examples = len(dataset_train)\n",
        "if n_train_examples * TARGET_EPOCHS < MIN_TARGET_EXAMPLES:\n",
        "    n_epochs = min(MAX_EPOCHS, MIN_TARGET_EXAMPLES // n_train_examples)\n",
        "elif n_train_examples * TARGET_EPOCHS > MAX_TARGET_EXAMPLES:\n",
        "    n_epochs = max(MIN_EPOCHS, MAX_TARGET_EXAMPLES // n_train_examples)\n",
        "\n",
        "n_billing_tokens_in_dataset = sum(min(MAX_TOKENS_PER_EXAMPLE, length) for length in convo_lens)\n",
        "print(f\"Dataset has ~{n_billing_tokens_in_dataset} tokens that will be charged for during training\")\n",
        "print(f\"By default, you'll train for {n_epochs} epochs on this dataset\")\n",
        "print(f\"By default, you'll be charged for ~{n_epochs * n_billing_tokens_in_dataset} tokens\")\n",
        "\n",
        "# Calculate the estimated cost for fine-tuning\n",
        "cost_per_100k_tokens = 0.80  # Cost for every 100,000 tokens\n",
        "estimated_cost = ((n_epochs * n_billing_tokens_in_dataset) / 100000) * cost_per_100k_tokens\n",
        "print(f\"Estimated cost for fine-tuning: approximately ${estimated_cost:.2f}\") #I added this for actual cost based on current pricing"
      ],
      "metadata": {
        "id": "nzUsnPj-Y00x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to save the dataset as a JSONL file\n",
        "def save_to_jsonl(conversations, file_path):\n",
        "    with open(file_path, 'w') as file:\n",
        "        for conversation in conversations:\n",
        "            json_line = json.dumps(conversation)\n",
        "            file.write(json_line + '\\n')\n",
        "\n",
        "# Specify the path where you want to save the JSONL file in your Google Drive\n",
        "training_jsonl_file_path = data_path+'-train.jsonl'\n",
        "testing_jsonl_file_path = data_path+'-test.jsonl'\n",
        "\n",
        "# Save the dataset to the specified file path\n",
        "save_to_jsonl(dataset_train, training_jsonl_file_path)\n",
        "save_to_jsonl(dataset_test, testing_jsonl_file_path)"
      ],
      "metadata": {
        "id": "GME1BGqXNGMQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Upload data to OpenAI for training"
      ],
      "metadata": {
        "id": "sDDjiHW51-_U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datetime import datetime\n",
        "# Format the current date and time in a human-readable format\n",
        "print(datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"))\n",
        "\n",
        "from openai import OpenAI\n",
        "\n",
        ")\n",
        "\n",
        "training_file_name = training_jsonl_file_path\n",
        "\n",
        "training_response = client.files.create(\n",
        "  file=open(training_file_name, \"rb\"),\n",
        "  purpose=\"fine-tune\"\n",
        ")\n",
        "training_file_id = training_response.id\n",
        "print(\"Training file id:\", training_file_id)\n",
        "\n",
        "validation_file_name = testing_jsonl_file_path\n",
        "\n",
        "validation_response = client.files.create(\n",
        "  file=open(validation_file_name, \"rb\"),\n",
        "  purpose=\"fine-tune\"\n",
        ")\n",
        "validation_file_id = validation_response.id\n",
        "\n",
        "#Gives training file id\n",
        "print(\"Validation file id:\", validation_file_id)\n",
        "\n",
        "# 2024-01-05 8pm: Training file id: file-TMQuBaSWdCa2MKMa7iDA8Klm\n",
        "\n",
        "# 2024-01-05 10pm: Training file id: file-px88PEb4t11QC3rLY7FEFZCm\n",
        "\n",
        "# 2023-01-08 11pm: Training file id: file-oF5tFlyiWMDWVDMmmEQRXUn5\n",
        "\n",
        "# 2024-01-09 18:38:00\n",
        "# Training file id: file-RVsnOPc81XyTloaJIi8A2qaA\n",
        "# Validation file id: file-oTjXdDxeiAquIBJIprcI0258"
      ],
      "metadata": {
        "id": "d403QpVVN3BU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Create Fine-Tuning Job\n",
        "suffix_name = \"isaac-email-1\"\n",
        "\n",
        "fine_tuned_model_training_response = client.fine_tuning.jobs.create(\n",
        "    training_file=training_file_id,\n",
        "    model=\"gpt-3.5-turbo\",\n",
        "    suffix=suffix_name,\n",
        "    validation_file=validation_file_id\n",
        ")\n",
        "\n",
        "job_id = fine_tuned_model_training_response.id\n",
        "\n",
        "print(fine_tuned_model_training_response)"
      ],
      "metadata": {
        "id": "SlZEZYGTOY-L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datetime import datetime\n",
        "\n",
        "def UNIX_timestamp_to_formatted_datetime(date):\n",
        "    if date == None:\n",
        "        return None\n",
        "    unix_timestamp = int(date)  # Convert to integer and then to seconds\n",
        "    date_time_obj = datetime.utcfromtimestamp(unix_timestamp)\n",
        "    # Format the datetime object as a string\n",
        "    formatted_date = date_time_obj.strftime('%Y-%m-%d %H:%M:%S')\n",
        "    return formatted_date\n",
        "\n",
        "!pip install openai\n",
        "from openai import OpenAI\n",
        "\n",
        "client = OpenAI(\n",
        "    api_key=\"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\",\n",
        ")\n",
        "\n",
        "#paste ft fine-tune file id from line \"message\": \"Created fine-tune: ft-XXXXXXXXX\"\n",
        "list_of_jobs_response = client.fine_tuning.jobs.list(limit=10)\n",
        "print('Jobs found:',str(len(list_of_jobs_response.data)))\n",
        "print()\n",
        "# for x in list_of_jobs_response:\n",
        "#   print_fields(x)\n",
        "\n",
        "events = list_of_jobs_response.data\n",
        "events.reverse()\n",
        "\n",
        "for event in events:\n",
        "    if not event.error:\n",
        "        print(event)\n",
        "        print('created: ',UNIX_timestamp_to_formatted_datetime(event.created_at))\n",
        "        print('finished: ',UNIX_timestamp_to_formatted_datetime(event.finished_at))\n",
        "        print()"
      ],
      "metadata": {
        "id": "1Xyzue2SPYaZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install openai\n",
        "from openai import OpenAI\n",
        "\n",
        "client = OpenAI(\n",
        "    api_key=\"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\",\n",
        ")"
      ],
      "metadata": {
        "id": "RNnksMsgjPdr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "file_list = client.files.list()\n",
        "\n",
        "file_names = {}\n",
        "file_sizes = {}\n",
        "for x in file_list:\n",
        "    print(x)\n",
        "    print(x.id)"
      ],
      "metadata": {
        "id": "eJhdBm2N4WbS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "file_list = client.files.list()\n",
        "\n",
        "file_names = {}\n",
        "file_sizes = {}\n",
        "for x in file_list:\n",
        "    file_sizes[x.id] = x.bytes\n",
        "    file_names[x.id] = x.filename\n",
        "\n",
        "#paste ft fine-tune file id from line \"message\": \"Created fine-tune: ft-XXXXXXXXX\"\n",
        "list_of_jobs_response = client.fine_tuning.jobs.list(limit=10)\n",
        "print('Jobs found:',str(len(list_of_jobs_response.data)))\n",
        "print()\n",
        "\n",
        "events = list_of_jobs_response.data\n",
        "events.reverse()\n",
        "events = sorted(events, key=lambda x: x.created_at)\n",
        "\n",
        "for i,event in enumerate(events):\n",
        "    if not event.error:\n",
        "        print(i)\n",
        "        print('fine_tuned_model:',event.fine_tuned_model)\n",
        "        print('training file name:',file_names[event.training_file])\n",
        "        print('training file size:',\"{:,}\".format(file_sizes[event.training_file]))\n",
        "        print()"
      ],
      "metadata": {
        "id": "zSg6nUN_7XEo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def print_fields(obj, indent=0):\n",
        "    # Determine the indentation\n",
        "    indentation = ' ' * indent\n",
        "\n",
        "    # If obj is a dictionary, iterate over its items\n",
        "    if isinstance(obj, dict):\n",
        "        for key, value in obj.items():\n",
        "            print(f\"{indentation}{key}:\")\n",
        "            print_fields(value, indent + 2)\n",
        "    # If obj is an object, iterate over its attributes\n",
        "    elif hasattr(obj, '__dict__'):\n",
        "        for key, value in obj.__dict__.items():\n",
        "            print(f\"{indentation}{key}:\")\n",
        "            print_fields(value, indent + 2)\n",
        "    # Otherwise, just print the value\n",
        "    else:\n",
        "        print(f\"{indentation}{obj}\")\n",
        "\n",
        "model_selector = 4\n",
        "print_fields(events[model_selector])\n",
        "\n",
        "#retrieve fine-tune model\n",
        "fine_tuned_model_id = events[model_selector].fine_tuned_model\n",
        "print(\"\\nFine-tuned model id:\", fine_tuned_model_id)"
      ],
      "metadata": {
        "id": "Lca9qoZ_ib_S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "events[-1].result_files[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "e3779pQqb2lf",
        "outputId": "29e6baa7-4782-4e7a-9e64-9c08d3ffb4f7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'file-gCEk7yFpbxRIjeQC5FWbA8tD'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "content = client.files.retrieve_content(\"file-B6Qby1PY4Pf3j9qIEVGiiBCG\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BfjpC18_cblh",
        "outputId": "2e999e41-1ca0-4c76-8b76-008e9f4dc1d1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-9-7bf79dde8804>:1: DeprecationWarning: The `.content()` method should be used instead\n",
            "  content = client.files.retrieve_content(\"file-B6Qby1PY4Pf3j9qIEVGiiBCG\")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Single completion mode"
      ],
      "metadata": {
        "id": "ZOT7ErziM7g8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Test it out!\n",
        "test_messages = []\n",
        "\n",
        "system_message = (\"You are a computer science graduate in the Marine Corps\"\n",
        "                    \" Be polite and formal. Do not apologize. Use correct grammar and avoid logic fallacies.\")\n",
        "test_messages.append({\"role\": \"system\", \"content\": system_message})\n",
        "user_message = \"What did you do today?\"\n",
        "test_messages.append({\"role\": \"user\", \"content\": user_message})\n",
        "\n",
        "from openai import OpenAI\n",
        "\n",
        "client = OpenAI(\n",
        "    api_key=\"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\",\n",
        ")\n",
        "\n",
        "# fine_tuned_model_id = 'ft:gpt-3.5-turbo-0613:university-of-california-santa-barbara:isaac-sms-bot:8ds3OPKx'\n",
        "fine_tuned_model_id = events[model_selector].fine_tuned_model\n",
        "\n",
        "#OpenAI Chat Completions\n",
        "response = client.chat.completions.create(\n",
        "    model=fine_tuned_model_id, #can test it against gpt-3.5-turbo to see difference\n",
        "    messages=test_messages,\n",
        "    temperature=0.1,\n",
        "    max_tokens=500\n",
        ")\n",
        "print(response.choices[0].message)"
      ],
      "metadata": {
        "id": "AJ2FtN_IVyxC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response_text = response.choices[0].text\n",
        "\n",
        "# Find the first occurrence of 'END' and slice the string up to that point\n",
        "end_index = response_text.find('END')\n",
        "if end_index != -1:\n",
        "    response_text = response_text[:end_index]\n",
        "\n",
        "# Clean up the response text by removing leading/trailing white space\n",
        "response_text = response_text.strip()\n",
        "\n",
        "print(response_text)"
      ],
      "metadata": {
        "id": "KEvjkQCcRJ9p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Conversation Mode"
      ],
      "metadata": {
        "id": "Rch8ad3lNAEV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install openai\n",
        "from openai import OpenAI\n",
        "\n",
        "client = OpenAI(\n",
        "    api_key=\"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\",\n",
        ")"
      ],
      "metadata": {
        "id": "d_sgzcGq276H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "file_list = client.files.list()\n",
        "\n",
        "file_names = {}\n",
        "file_sizes = {}\n",
        "for x in file_list:\n",
        "    file_sizes[x.id] = x.bytes\n",
        "    file_names[x.id] = x.filename\n",
        "\n",
        "#paste ft fine-tune file id from line \"message\": \"Created fine-tune: ft-XXXXXXXXX\"\n",
        "list_of_jobs_response = client.fine_tuning.jobs.list(limit=10)\n",
        "\n",
        "events = list_of_jobs_response.data\n",
        "events.reverse()\n",
        "events = sorted(events, key=lambda x: x.created_at)\n",
        "\n",
        "model_selector = 4\n",
        "event = events[model_selector]\n",
        "print('fine_tuned_model:',event.fine_tuned_model)\n",
        "print('training file name:',file_names[event.training_file])\n",
        "print('training file size:',\"{:,}\".format(file_sizes[event.training_file]))\n",
        "print('trained_tokes:',\"{:,}\".format(event.trained_tokens))\n",
        "print('n_epochs:',event.hyperparameters.n_epochs)\n",
        "print()\n",
        "\n",
        "#retrieve fine-tune model\n",
        "fine_tuned_model_id = events[model_selector].fine_tuned_model"
      ],
      "metadata": {
        "id": "IOK4YJ7gDakq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Test it out!\n",
        "chat_messages = []\n",
        "\n",
        "system_message = (\"You are a computer science graduate in the Marine Corps\"\n",
        "                      \" Be polite and formal. Do not apologize. Use correct grammar and avoid logic fallacies.\")\n",
        "chat_messages.append({\"role\": \"system\", \"content\": system_message})\n",
        "\n",
        "user_message = input(\"user     : \")  # User input\n",
        "\n",
        "for _ in range(10):\n",
        "\n",
        "    chat_messages.append({\"role\": \"user\", \"content\": user_message})\n",
        "\n",
        "    #OpenAI Chat Completions\n",
        "    response = client.chat.completions.create(\n",
        "    model=fine_tuned_model_id, #can test it against gpt-3.5-turbo to see difference\n",
        "    messages=chat_messages,\n",
        "    temperature=0.01,\n",
        "    max_tokens=500\n",
        "    )\n",
        "    response_text = response.choices[0].message.content\n",
        "    chat_messages.append({\"role\": \"assistant\", \"content\": response_text})\n",
        "    print(\"assistant:\",response.choices[0].message.content)\n",
        "\n",
        "    user_message = input(\"user     : \")  # User input\n",
        "    if user_message == 'end':\n",
        "        print('Conversation ended by user')\n",
        "        break"
      ],
      "metadata": {
        "id": "h3j3NpGcGZ2k"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
